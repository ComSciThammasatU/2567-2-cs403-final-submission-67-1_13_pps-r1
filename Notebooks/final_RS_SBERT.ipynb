{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98600640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 1: Configuration ────────────────────────────────────────────\n",
    "COURSE_FILE = \"../Data/course_data.xlsx\"\n",
    "JOBS_FILE   = \"../Data/job_data.xlsx\"\n",
    "SHEET_NAME        = \"Sheet1\"\n",
    "\n",
    "# ─── Learner Profile ─────────────────────────────────────────────────\n",
    "DESIRED_JOB       = \"\"\n",
    "COURSES_COMPLETED = [ \n",
    "]\n",
    "# ─── Embedding Params & Stopwords ────────────────────────────────────\n",
    "# CHUNK_SIZE        = 256\n",
    "thai_stopwords    = {\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f75918d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ninen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ─── Imports & Helper Functions ─────────────────────────────────\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(str(text))\n",
    "    return [w for w in tokens if w not in thai_stopwords]\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = text.replace(\"\\xa0\", \" \")\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c678906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Load & Preprocess Data ───────────────────────────────────────\n",
    "courses = pd.read_excel(COURSE_FILE, sheet_name=SHEET_NAME)\n",
    "jobs    = pd.read_excel(JOBS_FILE)\n",
    "jobs    = jobs.rename(columns={\"Job Title\":\"JOB_NAME\", \"Job Description\":\"REQUIREMENTS\"})\n",
    "\n",
    "courses[\"PROCESSED_DESC\"] = courses[\"C_Description\"].apply(preprocess_text)\n",
    "jobs   [\"PROCESSED_REQ\"]  = jobs  [\"REQUIREMENTS\"].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2588e9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ninen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 4: Load SBERT Model ───────────────────────────────────────\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# pick your SBERT model\n",
    "MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load SentenceTransformer\n",
    "sbert_model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "\n",
    "# enforce the 512-token max\n",
    "sbert_model.tokenizer.model_max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1efd3bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.32it/s]\n",
      "Batches: 100%|██████████| 5/5 [00:02<00:00,  1.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 5: Batch-encode with SBERT ─────────────────────────────────\n",
    "# prepare clean text\n",
    "jobs   ['TEXT_CLEAN']   = jobs   ['PROCESSED_REQ'].apply(lambda toks: ' '.join(toks))\n",
    "courses['TEXT_CLEAN']   = courses['PROCESSED_DESC'].apply(lambda toks: ' '.join(toks))\n",
    "\n",
    "# encode in one go\n",
    "jobs_embs    = sbert_model.encode(\n",
    "    jobs['TEXT_CLEAN'].tolist(),\n",
    "    batch_size=32,\n",
    "    convert_to_numpy=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "courses_embs = sbert_model.encode(\n",
    "    courses['TEXT_CLEAN'].tolist(),\n",
    "    batch_size=32,\n",
    "    convert_to_numpy=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# attach embeddings\n",
    "jobs   ['EMBEDDING']   = list(jobs_embs)\n",
    "courses['EMBEDDING']   = list(courses_embs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7053b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 6a (after you’ve computed EMBEDDING columns) ─────────────────\n",
    "from sklearn.preprocessing import normalize as sk_normalize\n",
    "\n",
    "# stack into a 2D array, normalize row-wise, then unpack back to lists\n",
    "jobs_embs = np.stack(jobs[\"EMBEDDING\"].to_list())          # shape (n_jobs, dim)\n",
    "courses_embs = np.stack(courses[\"EMBEDDING\"].to_list())    # shape (n_courses, dim)\n",
    "\n",
    "jobs_embs    = sk_normalize(jobs_embs,    axis=1)          # each row → unit length\n",
    "courses_embs = sk_normalize(courses_embs, axis=1)\n",
    "\n",
    "jobs[\"EMBEDDING\"]    = list(jobs_embs)                     # back into DataFrame\n",
    "courses[\"EMBEDDING\"] = list(courses_embs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13a65480",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_profile = {\n",
    "    \"desired_job\": \"โปรแกรมเมอร์\", \n",
    "    \"courses_completed\": [\n",
    "        \"จริยธรรมเพื่อการดำเนินชีวิต\",\n",
    "        \"สังคมไทยและโลกาภิวัตน์\",\n",
    "        \"พฤติกรรมสุขภาพ\",\n",
    "        \"การอ่านและการเขียนภาษาอังกฤษทั่วไป\",\n",
    "        \"การบริหารงานสาธารณสุข\",\n",
    "        \"ชีววิทยาเบื้องต้น\",\n",
    "        \"การพัฒนาทักษะการคิด\",\n",
    "        \"การสร้างเสริมสุขภาพ\",\n",
    "        \"การวิจัยทางสาธารณสุข\",\n",
    "        \"สังคมวิทยาและมานุษยวิทยาสาธารณสุข\",\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9844455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your desired job is:\n",
      "โปรแกรมเมอร์\n",
      "\n",
      "Recommended Courses:\n",
      "                                C_Name  \\\n",
      "10              การเขียนโปรแกรมขั้นสูง   \n",
      "43           การเขียนโปรแกรมควบคุมระบบ   \n",
      "8         โครงสร้างข้อมูลและอัลกอริทึม   \n",
      "20             ระบบสนับสนุนการตัดสินใจ   \n",
      "33           การวิเคราะห์และออกแบบระบบ   \n",
      "31            การเขียนโปรแกรมเชิงวัตถุ   \n",
      "21          โครงงานวิทยาการคอมพิวเตอร์   \n",
      "3       คอมพิวเตอร์สารสนเทศขั้นพื้นฐาน   \n",
      "16          การพัฒนาซอฟต์แวร์เชิงวัตถุ   \n",
      "121  คอมพิวเตอร์และสารสนเทศขั้นพื้นฐาน   \n",
      "\n",
      "                                         C_Description  SIMILARITY  \n",
      "10   แนวคิดในการเขียนโปรแกรมด้วยภาษาคอมพิวเตอร์ระดั...    0.686648  \n",
      "43   ฮาร์ดแวร์และโปรแกรมควบคุมระบบ ชุดคำสั่งโครงสร้...    0.655778  \n",
      "8     ศึกษาเกี่ยวกับโครงสร้างข้อมูลและอัลกอริทึมแบบ...    0.635257  \n",
      "20   ศึกษารูปแบบ ประเภท เทคนิคการทำเหมืองข้อมูล การ...    0.632573  \n",
      "33    ความหมาย องค์ประกอบของระบบสารสนเทศ วัฎจักรการ...    0.632058  \n",
      "31     หลักการเขียนโปรแกรมเชิงวัตถุ การออกแบบและพัฒ...    0.620563  \n",
      "21   ค้นคว้าและเลือกหัวข้อโครงงานที่เกี่ยวข้องกับวิ...    0.614618  \n",
      "3    ความรู้เบื้งต้นเกี่ยวกับคอมพิวเตอร์และเทคโนโลย...    0.608021  \n",
      "16   แนวคิดเชิงวัตถุ กระบวนการพัฒนาซอฟต์แวร์ การวิเ...    0.606979  \n",
      "121  ความรู้เบื้องต้นเกี่ยวกับคอมพิวเตอร์และเทคโนโล...    0.605112  \n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 6b: Filter, dedupe & recommend ───────────────────────────────\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1) Exclude courses the learner has already taken\n",
    "candidates = courses[~courses[\"C_Name\"].isin(learner_profile['courses_completed'])].copy()\n",
    "\n",
    "# 2) Compute similarity to the desired-job vector\n",
    "desired_vec = jobs.loc[jobs[\"JOB_NAME\"] == (learner_profile['desired_job']), \"EMBEDDING\"].iloc[0]\n",
    "candidates[\"SIMILARITY\"] = candidates[\"EMBEDDING\"].apply(\n",
    "    lambda v: cosine_similarity([desired_vec], [v])[0][0]\n",
    ")\n",
    "\n",
    "candidates_unique = candidates.drop_duplicates(subset=\"C_Name\", keep=\"first\")\n",
    "candidates_unique = candidates_unique.sort_values(['SIMILARITY'], ascending=False).head(10)\n",
    "\n",
    "print(\"Your desired job is:\")\n",
    "print(learner_profile['desired_job'])\n",
    "\n",
    "print(\"\\nRecommended Courses:\")\n",
    "print(candidates_unique[[\"C_Name\", \"C_Description\", \"SIMILARITY\"]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
